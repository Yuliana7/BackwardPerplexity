{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 14283.92M tokens, resulting in 714195k examples.\n"
     ]
    }
   ],
   "source": [
    "from modules import TokenTexth5\n",
    "from modules import tokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "toke_mine = tokenizer.get_tokenizer(m_path='modules/tokenizers/de_tokenizer',m_name='greek')\n",
    "data = TokenTexth5('../bigdrive/vassilis/deutsch/deutsch.h5',40,backwards=True)\n",
    "\n",
    "phrase = toke_mine.detokenize(data[0][0])\n",
    "answer = toke_mine.detokenize(data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase :  abrina Kampmann ist geschockt! Ein Unbekannter war nachts in ihrem Schlafzimmer, hat sie gefilmt und ihr sogar eine Haarsträhne abgeschnitten. Der Polizei sind die Hände gebunden, denn es gibt keinerlei \n",
      " answer :  Sabrina Kampmann ist geschockt! Ein Unbekannter war nachts in ihrem Schlafzimmer, hat sie gefilmt und ihr sogar eine Haarsträhne abgeschnitten. Der Polizei sind die Hände gebunden, denn es gibt\n"
     ]
    }
   ],
   "source": [
    "print(\"phrase : \",phrase,\"\\n answer : \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data : Οι\n",
      ". παρελθόν το με σχέση καμία έχει μην να οποίο το, περιβάλλον πολιτικό ένα ξαφνικά δούμε ναόμαστε ονειρευ όλοι όπου, σκηνικό πολιτικό στο και συνέβαινε να αυτό Μακάρι. κοινωνία σαν άκρη μια έτσι βρούμε, answer :  όταν ειδικά κοινωνίας της αυτής δημιουργίας της σκοπούςικούς αρχ τους μεδουν συνά δεν οποία τα πράγματαλογα παράειςβάλ επι να μπορεί πράγματα ταξεις ελέγ να προσπάθεια στην εφόσον συμβεί να εύκολο πολύ είναι αυτό και\n",
      "data : Οι\n",
      ". παρελθόν το με σχέση καμία έχει μην να οποίο το, περιβάλλον πολιτικό ένα ξαφνικά δούμε ναόμαστε ονειρευ όλοι όπου, σκηνικό πολιτικό στο και συνέβαινε να αυτό Μακάρι. κοινωνία σαν άκρη μια έτσι βρούμε, answer :  όταν ειδικά κοινωνίας της αυτής δημιουργίας της σκοπούςικούς αρχ τους μεδουν συνά δεν οποία τα πράγματαλογα παράειςβάλ επι να μπορεί πράγματα ταξεις ελέγ να προσπάθεια στην εφόσον συμβεί να εύκολο πολύ είναι αυτό και\n"
     ]
    }
   ],
   "source": [
    "print(f'data : {phrase}, answer : {answer}')\n",
    "print(f'data : {toke_mine.detokenize(toke_mine.tokenize(phrase))}, answer : {toke_mine.detokenize(toke_mine.tokenize(answer))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'είσαι μαλάκας, προφανώς, αλλά δεν είσαι και τόσο μαλάκας'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toke_mine.detokenize(toke_mine.tokenize('είσαι μαλάκας, προφανώς, αλλά δεν είσαι και τόσο μαλάκας'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file_path, chunk_size=10*1024*1024):  # Default chunk size is 10MB\n",
    "    \"\"\"\n",
    "    Generator to yield chunks of text from a large file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:  # End of file\n",
    "                break\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import tokenizer\n",
    "from transformers import GPT2TokenizerFast, AutoTokenizer\n",
    "\n",
    "toke_gpt2 =tokenizer.get_tokenizer(m_name='gpt2')\n",
    "\n",
    "toke_mine= toke_gpt2.train_new_from_iterator(read_in_chunks('hellas/hellas.txt'),vocab_size=50257)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [858, 469, 626, 259, 657, 594, 620, 469, 352, 554, 14, 984, 958, 923, 922, 919], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [858, 469, 626, 259, 657, 594, 620, 469, 352, 554, 14, 984, 958, 923, 922, 919], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(frompath('Gravida neque convallis a cras semper auctor neque vitae tempus. Dis parturient montes nascetur ridicul'))\n",
    "print(toke_mine('Gravida neque convallis a cras semper auctor neque vitae tempus. Dis parturient montes nascetur ridicul'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hellas\\\\tokenizer_config.json',\n",
       " 'hellas\\\\special_tokens_map.json',\n",
       " 'hellas\\\\vocab.json',\n",
       " 'hellas\\\\merges.txt',\n",
       " 'hellas\\\\added_tokens.json',\n",
       " 'hellas\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toke_mine.save_pretrained('hellas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frompath =GPT2TokenizerFast.from_pretrained('hellas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import TokenTextBOS\n",
    "from modules import tokenizer\n",
    "import numpy as np\n",
    "from pt_to_h5 import make_h5_from_dataset\n",
    "\n",
    "toke = tokenizer.get_tokenizer(m_path='modules/tokenizers/gr_tokenizer',m_name='greek')\n",
    "\n",
    "destination_path = 'datavol/vassilis/hellas/hellas.h5'\n",
    "\n",
    "rng = np.random.default_rng(42) \n",
    "\n",
    "motherDataset = TokenTextBOS(h5_file=destination_path, attn_length=64, backwards=False)\n",
    "indices = np.arange(len(motherDataset))\n",
    "rng.shuffle(indices)\n",
    "motherDataset = Subset(motherDataset, list(indices)) # Shuffled dataset\n",
    "\n",
    "\n",
    "# To keep it constant even if switching batch_size, I take batch_size=250\n",
    "val_inds = valid_steps*250\n",
    "val_range = range(len(motherDataset)-val_inds,len(motherDataset)) # Validation, last portion of dataset\n",
    "keep_range = range(len(motherDataset)-val_inds) # Training, first portion of dataset\n",
    "\n",
    "del indices\n",
    "#Whether backwards or forwards, its the individual examples that are flipped, not the dataset. So same thing for both !\n",
    "train_dataset = Subset(motherDataset, keep_range)\n",
    "val_dataset = Subset(motherDataset, val_range)\n",
    "\n",
    "print('Number 10 : ', train_dataset[10])\n",
    "print('Number 100 : ', train_dataset[11])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
