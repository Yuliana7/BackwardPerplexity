{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import tokenizer\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "toke_base = AutoTokenizer.from_pretrained('gpt2',use_fast=True)\n",
    "\n",
    "\n",
    "def read_in_chunks(file_path, chunk_size=10*1024*1024):  # Default chunk size is 10MB\n",
    "    \"\"\"\n",
    "    Generator to yield chunks of text from a large file.\n",
    "    TODO : Add support for .txt files which are not unified\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:  # End of file\n",
    "                break\n",
    "            yield chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('datatest/it_tokenizer/tokenizer_config.json',\n",
       " 'datatest/it_tokenizer/special_tokens_map.json',\n",
       " 'datatest/it_tokenizer/vocab.json',\n",
       " 'datatest/it_tokenizer/merges.txt',\n",
       " 'datatest/it_tokenizer/added_tokens.json',\n",
       " 'datatest/it_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .txt dataset (full dataset in ht txt file, for now.)\n",
    "txt_path = 'datatest/it.txt'\n",
    "toke_mine= toke_base.train_new_from_iterator(read_in_chunks(txt_path),vocab_size=50257)\n",
    "toke_path = os.path.join(os.path.dirname(txt_path), f'{os.path.basename(txt_path).split(\".\")[0]}_tokenizer')\n",
    "toke_mine.save_pretrained(toke_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 0.00M tokens, resulting in 0k examples.\n",
      "Dataset contains 0.00M tokens, resulting in 0k examples.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from modules import TokenTextBOS\n",
    "frompath =AutoTokenizer.from_pretrained('modules/tokenizers/fr_tokenizer')\n",
    "datatest = TokenTextBOS('test_h5/testdata.h5',attn_length=20,stride=20,backwards=False)\n",
    "datatest_b = TokenTextBOS('test_h5/testdata.h5', attn_length=20, stride=20,backwards=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(datatest))\n",
    "print(len(datatest_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD\n",
      "<|endoftext|>L’année 1866 fut marquée par un événement bizarre, un phénomène inexpliqué et inexplicable\n",
      "L’année 1866 fut marquée par un événement bizarre, un phénomène inexpliqué et inexplicable que\n",
      "<|endoftext|> personne n’a sans doute oublié. Sans parler des rumeurs qui agitaient les populations des ports\n",
      " personne n’a sans doute oublié. Sans parler des rumeurs qui agitaient les populations des ports et\n",
      "BACKWARD\n",
      "<|endoftext|> quelicable inexp etliqué inexp phénomène un, bizarre événement un par marquée fut66 18année’\n",
      " quelicable inexp etliqué inexp phénomène un, bizarre événement un par marquée fut66 18année’L\n",
      "<|endoftext|> et ports des populations lesaient agit qui rumeurs des parler Sans. oublié doute sansa’ n\n",
      " et ports des populations lesaient agit qui rumeurs des parler Sans. oublié doute sansa’ n personne\n"
     ]
    }
   ],
   "source": [
    "frompath.bos_token\n",
    "dataloa = DataLoader(datatest,batch_size=1)\n",
    "dataloa_b = DataLoader(datatest_b,batch_size=1)\n",
    "\n",
    "print('FORWARD')\n",
    "for i in range(2):   \n",
    "    print(frompath.decode(datatest[i][0].squeeze()))\n",
    "    print(frompath.decode(datatest[i][1].squeeze()))\n",
    "\n",
    "print('BACKWARD')\n",
    "for i in range(2):\n",
    "    print(frompath.decode(datatest_b[i][0].squeeze()))\n",
    "    print(frompath.decode(datatest_b[i][1].squeeze()))\n",
    "\n",
    "# print(datatest[1])\n",
    "# print(datatest_b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|><|endoftext|>!\"#$<|endoftext|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frompath.decode(torch.tensor([0,0,1,2,3,4,0],dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
